import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.model_selection import TimeSeriesSplit
from sklearn.preprocessing import StandardScaler
from sklearn.base import clone
import logging
import talib # Assuming talib is installed, otherwise use pandas_ta or manual calc
# If talib is not available, simple pandas implementations are provided in _calculate_features

class AdvancedSignalGenerator:
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.scaler = StandardScaler()
        self.is_trained = False
        
        # --- THE SOLUTION: REGULARIZED MODELS ---
        # 1. Random Forest: Limits depth to prevents memorizing noise
        self.rf_model = RandomForestClassifier(
            n_estimators=100,
            max_depth=5,              # Critical: Prevents unlimited complex rules
            min_samples_leaf=0.05,    # Critical: Requires 5% of data to form a rule
            max_features='sqrt',
            class_weight='balanced',  # Handles rare market events better
            random_state=42,
            n_jobs=-1
        )

        # 2. Gradient Boosting: Slow learning rate with subsampling
        self.gb_model = GradientBoostingClassifier(
            n_estimators=100,
            max_depth=3,              # Shallow trees are better for boosting
            learning_rate=0.05,       # Slower learning prevents overfitting
            subsample=0.8,            # Train on random 80% subset each iteration
            min_samples_leaf=0.05,
            random_state=42
        )

    def _calculate_features(self, df):
        """
        Robust feature engineering.
        Calculates ~15-20 key technical indicators.
        """
        data = df.copy()

        # Normalize column names (handle both 'Close' and 'close')
        data.columns = [col.capitalize() if col.lower() in ['open', 'high', 'low', 'close', 'volume'] else col for col in data.columns]

        # Ensure sufficient data length
        if len(data) < 50:
            return pd.DataFrame()

        # Verify required columns exist
        required = ['Close', 'High', 'Low', 'Volume']
        missing = [col for col in required if col not in data.columns]
        if missing:
            self.logger.debug(f"Missing columns: {missing}")
            return pd.DataFrame()

        # 1. Trend Indicators
        # MACD
        data['EMA_12'] = data['Close'].ewm(span=12, adjust=False).mean()
        data['EMA_26'] = data['Close'].ewm(span=26, adjust=False).mean()
        data['MACD'] = data['EMA_12'] - data['EMA_26']
        data['MACD_Signal'] = data['MACD'].ewm(span=9, adjust=False).mean()
        
        # SMAs
        data['SMA_20'] = data['Close'].rolling(window=20).mean()
        data['SMA_50'] = data['Close'].rolling(window=50).mean()
        data['Trend_SMA'] = np.where(data['SMA_20'] > data['SMA_50'], 1, -1)

        # 2. Momentum Indicators
        # RSI (Relative Strength Index)
        delta = data['Close'].diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
        rs = gain / loss
        data['RSI'] = 100 - (100 / (1 + rs))

        # 3. Volatility Indicators
        # Bollinger Bands
        data['BB_Upper'] = data['SMA_20'] + 2 * data['Close'].rolling(window=20).std()
        data['BB_Lower'] = data['SMA_20'] - 2 * data['Close'].rolling(window=20).std()
        data['BB_Width'] = (data['BB_Upper'] - data['BB_Lower']) / data['SMA_20']
        
        # ATR (Average True Range) - Simplified
        data['TR'] = np.maximum(
            (data['High'] - data['Low']), 
            np.maximum(
                abs(data['High'] - data['Close'].shift(1)), 
                abs(data['Low'] - data['Close'].shift(1))
            )
        )
        data['ATR'] = data['TR'].rolling(window=14).mean()

        # 4. Volume Indicators
        data['Vol_SMA'] = data['Volume'].rolling(window=20).mean()
        data['Rel_Vol'] = data['Volume'] / data['Vol_SMA']

        # 5. Returns (Target creation helper)
        data['Return_1d'] = data['Close'].pct_change()
        data['Return_5d'] = data['Close'].pct_change(5)
        
        # Drop NaN values generated by windows
        data.dropna(inplace=True)
        
        # Select Features for ML
        features = [
            'MACD', 'MACD_Signal', 'RSI', 'BB_Width', 'ATR', 
            'Rel_Vol', 'Return_1d', 'Return_5d', 'Trend_SMA'
        ]
        
        return data[features]

    def _create_targets(self, df):
        """
        Create target labels:
        1 (Buy) if next 5 days return > 1%
        -1 (Sell) if next 5 days return < -1%
        0 (Hold) otherwise
        """
        # Normalize column names
        data = df.copy()
        data.columns = [col.capitalize() if col.lower() in ['open', 'high', 'low', 'close', 'volume'] else col for col in data.columns]

        # Future 5-day return
        future_returns = data['Close'].shift(-5) / data['Close'] - 1
        
        targets = pd.Series(0, index=df.index)
        targets[future_returns > 0.01] = 1   # Buy
        targets[future_returns < -0.01] = -1 # Sell
        
        return targets.dropna()

    def train_models(self, historical_data_dict):
        """
        Train the ensemble models using the historical data dictionary.
        """
        self.logger.info("Starting regularized ML model training...")
        
        X_all = []
        y_all = []

        for symbol, df in historical_data_dict.items():
            if df.empty:
                continue
                
            # 1. Feature Engineering
            features = self._calculate_features(df)
            if features.empty:
                continue
                
            # 2. Target Creation
            # We must align features and targets because creating targets shifts data
            targets = self._create_targets(df)
            
            # Align indices
            common_index = features.index.intersection(targets.index)
            if len(common_index) < 50:
                continue
                
            X_all.append(features.loc[common_index].values)
            y_all.append(targets.loc[common_index].values)

        if not X_all:
            self.logger.error("No valid data found for training.")
            return

        # Combine all data
        X = np.concatenate(X_all)
        y = np.concatenate(y_all)

        # 3. Scaling
        X_scaled = self.scaler.fit_transform(X)

        self.logger.info(f"Training on {len(X)} samples with {X.shape[1]} features.")

        try:
            # 4. Train Random Forest
            self.rf_model.fit(X_scaled, y)
            rf_score = self.rf_model.score(X_scaled, y)
            
            # 5. Train Gradient Boosting
            self.gb_model.fit(X_scaled, y)
            gb_score = self.gb_model.score(X_scaled, y)

            self.is_trained = True
            self.logger.info(f"Training Completed. Training Accuracy -> RF: {rf_score:.3f}, GB: {gb_score:.3f}")
            
            # Sanity check for overfitting
            if rf_score > 0.95 or gb_score > 0.95:
                self.logger.warning("⚠️ Warning: Accuracy is dangerously high (>95%). Model may still be overfitting.")
            else:
                self.logger.info("✅ Model regularization appears effective (Accuracy < 95%).")

        except Exception as e:
            self.logger.error(f"Training failed: {str(e)}")
            self.is_trained = False

    def generate_ml_signal(self, symbol, prices_df):
        """
        Generate a signal for a live symbol.
        Returns dictionary: {'signal': float (-1 to 1), 'confidence': float (0 to 1)}
        """
        if not self.is_trained:
            self.logger.warning("Models not trained. Returning neutral signal.")
            return {'signal': 0.0, 'confidence': 0.0}

        try:
            # 1. Calculate Features for the specific symbol
            features = self._calculate_features(prices_df)
            if features.empty:
                return {'signal': 0.0, 'confidence': 0.0}

            # Get the very latest row (live market state)
            latest_features = features.iloc[[-1]].values
            latest_scaled = self.scaler.transform(latest_features)

            # 2. Get Probabilities from both models
            # Classes are likely [-1, 0, 1]. We need prob of -1 (Sell) and 1 (Buy)
            
            # Random Forest Probabilities
            rf_probs = self.rf_model.predict_proba(latest_scaled)[0]
            # Gradient Boosting Probabilities
            gb_probs = self.gb_model.predict_proba(latest_scaled)[0]
            
            # Helper to safely get prob for class index
            classes = self.rf_model.classes_
            
            def get_prob(probs, cls_label):
                if cls_label in classes:
                    idx = np.where(classes == cls_label)[0][0]
                    return probs[idx]
                return 0.0

            # 3. Ensemble Voting Logic
            # We average the "Buy" probability and "Sell" probability
            avg_buy_prob = (get_prob(rf_probs, 1) + get_prob(gb_probs, 1)) / 2
            avg_sell_prob = (get_prob(rf_probs, -1) + get_prob(gb_probs, -1)) / 2
            
            # 4. Construct Signal (-1.0 to 1.0)
            # If Buy Prob > Sell Prob, signal is positive.
            net_signal = avg_buy_prob - avg_sell_prob
            
            # Confidence is the magnitude of the dominant probability
            confidence = max(avg_buy_prob, avg_sell_prob)
            
            # 5. Thresholding (Noise Filter)
            if confidence < 0.4:  # If model isn't at least 40% sure of a direction
                net_signal = 0.0
            
            return {
                'signal': float(net_signal),
                'confidence': float(confidence),
                'details': {
                    'rf_buy': float(get_prob(rf_probs, 1)),
                    'gb_buy': float(get_prob(gb_probs, 1))
                }
            }

        except Exception as e:
            self.logger.error(f"Error generating signal for {symbol}: {e}")
            return {'signal': 0.0, 'confidence': 0.0}
